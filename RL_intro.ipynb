{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG/BVSNX+n7vXU42U1NOQh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vin136/100-days-of-DATA-SCIENCE/blob/main/RL_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D9TtJvbuIg35"
      },
      "outputs": [],
      "source": [
        "#Anatomy of an environment\n",
        "import random\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.steps_left = 10\n",
        "\n",
        "    def get_observation(self):\n",
        "        return [0.0, 0.0, 0.0]\n",
        "\n",
        "    def get_actions(self):\n",
        "        return [0, 1]\n",
        "\n",
        "    def is_done(self):\n",
        "        return self.steps_left == 0\n",
        "\n",
        "    def action(self, action):\n",
        "        if self.is_done():\n",
        "            raise Exception(\"Game is over\")\n",
        "        self.steps_left -= 1\n",
        "        return random.random()\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def step(self, env):\n",
        "        current_obs = env.get_observation()\n",
        "        actions = env.get_actions()\n",
        "        reward = env.action(random.choice(actions))\n",
        "        self.total_reward += reward\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9am-91TTKMUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PyATzGStKNJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment()\n",
        "agent = Agent()\n",
        "\n",
        "while not env.is_done():\n",
        "    agent.step(env)\n",
        "\n",
        "print(\"Total reward got: %.4f\" % agent.total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acAIZ0lPImhx",
        "outputId": "703e29e6-a50d-4083-bc3b-e77cb89c6a04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward got: 5.6868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.A set of actions that is allowed to be executed in the environment. Gym\n",
        "supports both discrete and continuous actions, as well as their combination.\n",
        "\n",
        "2.A method called step to execute an action, which returns the current\n",
        "observation, the reward, and the indication that the episode is over\n",
        "\n",
        "3.A method called reset, which returns the environment to its initial state\n",
        "and obtains the first observation"
      ],
      "metadata": {
        "id": "Vz-CyFo-LZIR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9D3Rw5jGM2xg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "total_reward = 0.0\n",
        "total_steps = 0\n",
        "obs = env.reset()\n",
        "\n",
        "while True:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    total_steps += 1\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HAOJG_dIoZC",
        "outputId": "0171fa1f-1081-41a0-f774-1f2a626732f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode done in 34 steps, total reward 34.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V iteration"
      ],
      "metadata": {
        "id": "BWd-cDWDO1LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset() if is_done else new_state\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    reward = self.rewards[(state, action, tgt_state)]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
        "                self.values[(state, action)] = action_value"
      ],
      "metadata": {
        "id": "-OzU0hIJLymS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()\n",
        "writer = collections.defaultdict(list)\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    agent.play_n_random_steps(100)\n",
        "    agent.value_iteration()\n",
        "\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer['reward'].append(reward)\n",
        "    if reward > best_reward:\n",
        "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(\"Solved in %d iterations!\" % iter_no)\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms3gsclPO5_D",
        "outputId": "da80059b-ebe5-4ba7-8dee-a31a8c21d49d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.150\n",
            "Best reward updated 0.150 -> 0.300\n",
            "Best reward updated 0.300 -> 0.400\n",
            "Best reward updated 0.400 -> 0.650\n",
            "Best reward updated 0.650 -> 0.700\n",
            "Best reward updated 0.700 -> 0.750\n",
            "Best reward updated 0.750 -> 0.800\n",
            "Best reward updated 0.800 -> 0.900\n",
            "Solved in 109 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize the values of all states, Vi\n",
        ", to some initial value (usually zero)\n",
        "2. For every state, s, in the MDP, perform the Bellman update:\n",
        "𝑉𝑉𝑠𝑠 ← max𝑎𝑎∑ 𝑝𝑝𝑎𝑎,𝑠𝑠→𝑠𝑠′(𝑟𝑟𝑠𝑠,𝑎𝑎 + 𝛾𝛾𝑉𝑉𝑠𝑠′) 𝑠𝑠′\n",
        "3. Repeat step"
      ],
      "metadata": {
        "id": "hb_LL3NRbGEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. our state space should be discrete and small enough to perform multiple\n",
        "iterations over all states.\n",
        "\n",
        "2. The second practical problem arises from the fact that we rarely know the\n",
        "transition probability for the actions and rewards matrix. \n",
        "\n",
        "3. the obvious answer to this issue\n",
        "is to use our agent's experience as an estimation for both unknowns. Rewards could\n",
        "be used as they are. We just need to remember what reward we got on the transition\n",
        "from s0\n",
        " to s1\n",
        " using action a, but to estimate probabilities, we need to maintain\n",
        "counters for every tuple (s0\n",
        ", s1\n",
        ", a) and normalize them."
      ],
      "metadata": {
        "id": "ZiN1rjdWbTJN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6CeHmV4PgHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall logic:\n",
        "\n",
        "\n",
        "in the loop, we play 100 random steps from\n",
        "the environment, populating the reward and transition tables. After those 100\n",
        "steps, we perform a value iteration loop over all states, updating our value table.\n",
        "Then we play several full episodes to check our improvements using the updated\n",
        "value table. If the average reward for those test episodes is above the 0.8 boundary,\n",
        "then we stop training. During the test episodes, we also update our reward and\n",
        "transition tables to use all data from the environment."
      ],
      "metadata": {
        "id": "PMyuDFafcDKM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgPf40ZHcGVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q iteration"
      ],
      "metadata": {
        "id": "cVxlQo1Vf4r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset() if is_done else new_state\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    reward = self.rewards[(state, action, tgt_state)]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
        "                self.values[(state, action)] = action_value\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FvHYbxEtf6bg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()\n",
        "writer = collections.defaultdict(list)\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    agent.play_n_random_steps(100)\n",
        "    agent.value_iteration()\n",
        "\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer['reward'].append(reward)\n",
        "    #writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(\"Solved in %d iterations!\" % iter_no)\n",
        "        break\n",
        "#writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3DSTzeTf-sq",
        "outputId": "2ed602c5-f552-4455-c4b2-b0a9f70ac802"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.350\n",
            "Best reward updated 0.350 -> 0.400\n",
            "Best reward updated 0.400 -> 0.450\n",
            "Best reward updated 0.450 -> 0.550\n",
            "Best reward updated 0.550 -> 0.700\n",
            "Best reward updated 0.700 -> 0.850\n",
            "Solved in 34 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsj37FSxgWqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}